<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>首页 on Machine Learning 学习笔记</title>
    <link>https://kingye.me/study-ml/</link>
    <description>Recent content in 首页 on Machine Learning 学习笔记</description>
    <generator>Hugo -- gohugo.io</generator><atom:link href="https://kingye.me/study-ml/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>基础</title>
      <link>https://kingye.me/study-ml/docs/appendix/interview/basic/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://kingye.me/study-ml/docs/appendix/interview/basic/</guid>
      <description>基础面试题 #  </description>
    </item>
    
    <item>
      <title>进阶</title>
      <link>https://kingye.me/study-ml/docs/appendix/interview/advanced/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://kingye.me/study-ml/docs/appendix/interview/advanced/</guid>
      <description>进阶面试题 #  </description>
    </item>
    
    <item>
      <title>4.1 教程</title>
      <link>https://kingye.me/study-ml/docs/appendix/tutorial/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://kingye.me/study-ml/docs/appendix/tutorial/</guid>
      <description>教程 #  基础 #  【视频】李宏毅 2021/2022 春机器学习课程 #  【视频】吴恩达机器学习系列课程 #  【视频】吴恩达深度学习 deeplearning.ai #  【视频】林轩田机器学习基石 #  【视频】机器学习技法（林轩田） #  Fafa-DL/Lhy_Machine_Learning #  李宏毅 2021 春季机器学习课程课件及作业
microsoft/ML-For-Beginners #  Azure Cloud Advocates at Microsoft are pleased to offer a 12-week, 26-lesson curriculum all about Machine Learning.
Avik-Jain/100-Days-Of-ML-Code #  MLEveryday/100-Days-Of-ML-Code #  机器学习 100 天
llSourcell/Learn_Machine_Learning_in_3_Months #  This is the code for &amp;ldquo;Learn Machine Learning in 3 Months&amp;rdquo; by Siraj Raval on Youtube</description>
    </item>
    
    <item>
      <title>4.3 关注项目</title>
      <link>https://kingye.me/study-ml/docs/appendix/attention/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://kingye.me/study-ml/docs/appendix/attention/</guid>
      <description>关注项目 #  机器学习 #  tensorflow/tensorflow #  scikit-learn/scikit-learn #  scikit-learn: machine learning in Python https://scikit-learn.org
php-ai/php-ml #  PHP-ML - Machine Learning library for PHP https://php-ml.org/
 深度学习 #  paddlepaddle/paddle #  Tencent/ncnn #  ncnn is a high-performance neural network inference framework optimized for the mobile platform
 机器人 #  gunthercox/ChatterBot #  ChatterBot is a machine learning, conversational dialog engine for creating chat bots https://chatterbot.</description>
    </item>
    
    <item>
      <title>LLM</title>
      <link>https://kingye.me/study-ml/docs/basic/lm/llm/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://kingye.me/study-ml/docs/basic/lm/llm/</guid>
      <description>LLM（大型语言模型） #  大语言模型 (英语：large language model，LLM) 是一种语言模型，由具有许多参数（通常数十亿个权重或更多）的人工神经网络组成，使用自监督学习或半监督学习对大量未标记文本进行训练。大型语言模型在 2018 年左右出现，并在各种任务中表现出色。
开源大语言模型 #   OpenLLaMA: openlm-research/open_llama  LLaMA (Large Language Model Meta AI)   Falcon: https://huggingface.co/tiiuae/falcon-7b  大模型应用工具 #   langchain-ai/langchain  microsoft/TaskMatrix  microsoft/semantic-kernel  Significant-Gravitas/Auto-GPT  reworkd/AgentGPT    BERT #  Bidirectional Encoder Representations from Transformers
google-research/bert 微调 #  预训练大语言模型的三种微调技术总结：
 fine-tuning  Fine-tuning 的基本思想是采用已经在大量文本上进行训练的预训练语言模型，然后在小规模的任务特定文本上继续训练它。   parameter-efficient fine-tuning  PEFT 在尽可能减少所需的参数和计算资源的情况下，实现对预训练语言模型的有效微调。 3 种技术  蒸馏(distillation)，它由 Hinton 等人于 2015 年引入。该方法涉及训练一个较小的模型来模仿一个较大的预训练模型的行为。预训练模型生成“教师”预测结果，然后用于训练较小的“学生”模型。通过这样做，学生模型可以从较大模型的知识中学习，而无需存储所有参数。 适配器训练(adapter training)，它由 Houlsby 等人于 2019 年引入。适配器是添加到预训练模型中的小型神经网络，用于特定任务的微调。这些适配器只占原始模型大小的一小部分，这使得训练更快，内存需求更低。适配器可以针对多种任务进行训练，然后插入到预训练模型中以执行新任务。 渐进收缩(progressive shrinking)，它由 Kaplan 等人于 2020 年引入。这种技术涉及在 fine-tuning 期间逐渐减小预训练模型的大小。从一个大模型开始，逐渐减少参数的数量，直到达到所需的性能。这种方法可以产生比从头开始训练的模型性能更好的小型模型。     prompt-tuning  只修改模型的输入，不需要大量计算资源    参考：预训练大语言模型的三种微调技术总结：fine-tuning、parameter-efficient fine-tuning 和 prompt-tuning</description>
    </item>
    
    <item>
      <title>ONNX</title>
      <link>https://kingye.me/study-ml/docs/basic/onnx/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://kingye.me/study-ml/docs/basic/onnx/</guid>
      <description>ONNX #  Open Neural Network Exchange
开放神经网络交换
 是微软和 Facebook 提出用来表示深度学习模型的开放格式 定义了一组和环境，平台均无关的标准格式，来增强各种 AI 模型的可交互性 无论你使用何种训练框架训练模型（比如 TensorFlow/Pytorch/OneFlow/Paddle），在训练完毕后你都可以将这些框架的模型统一转换为 ONNX 这种统一的格式进行存储  </description>
    </item>
    
    <item>
      <title>教程</title>
      <link>https://kingye.me/study-ml/docs/basic/ml/dl/tutorial/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://kingye.me/study-ml/docs/basic/ml/dl/tutorial/</guid>
      <description>教程 #  基础 #  《深度学习》(花书) #  MingchaoZhu/DeepLearning #  Python for《Deep Learning》，该书为《深度学习》(花书) 数学推导、原理剖析与源码级别代码实现
 进阶 #   </description>
    </item>
    
  </channel>
</rss>
